<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kanishk Jain</title>
  
  <meta name="author" content="Kanishk Jain">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kanishk Jain</name>
              </p>
              <p>I am a final year Master's student at <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a>, where I am co-advised by <a href="https://www.iiit.ac.in/people/faculty/vgandhi/">Prof. Vineet Gandhi</a> and <a href="https://www.iiit.ac.in/people/faculty/mkrishna/">Prof. K Madhava Krishna</a>. I am part of the <a href="https://cvit.iiit.ac.in/">Center for Visual Information Technology</a> lab, where I have worked on Visual Grounding, Language-Guided Autonomous Navigation, Multi-View Detection and Multi-Object Tracking. 
              </p>
              <p style="text-align:center">
                <a href="mailto:kanishk5991@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Kanishk_CV.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=NCVuTTAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/kanji95">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/kanji95/">Linkedin</a> &nbsp/&nbsp
                <a href="https://twitter.com/kanishkjain95">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/webpage-circle.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/webpage-circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in topics spanning: designing explainable NN based solution, mechanisms to avoid spurious correlations, improving the performance of NNs on out-of-distribution (OOD) data, multimodal learning, and low-resource learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
          <tr>
            <td style="padding:20px;width:5%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vln_2022_image'><img src='images/vln_2022.jpg' width="150" height="150" class="center"></div>
                <!-- <div class="two" id='acl_2022_image'><img src='images/acl_2022.jpg' class="center"></div> -->
              </div>
            </td>
            <td style="padding:10px;width:45%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2209.11972.pdf">
                <papertitle>Ground then Navigate: Language-guided Navigation in Dynamic Scenes</papertitle>
              </a>
              <br>
              <strong>Kanishk Jain</strong>, Varun Chhangani, Amogh Tiwari, K Madhava Krishna, Vineet Gandhi
              <br>
              <em>Arxiv</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2209.11972.pdf">pdf</a> / <a href="data/vln_2022.bib">bibtex</a>
              <p></p>
              <p>We investigate the Vision-and-Language Navigation problem in the context of autonomous driving in outdoor settings. We explicitly ground the navigable regions corresponding to the textual command and use them directly as guidance for the navigation stack.</p>
            </td>
          </tr>

        <tr>
          <td style="padding:20px;width:5%;vertical-align:middle">
            <div class="one">
              <div class="two" id='mvd_image'><img src='images/mvd.jpg' width="180" height="100" class="center"></div>
              <!-- <div class="two" id='acl_2022_image'><img src='images/acl_2022.jpg' class="center"></div> -->
            </div>
          </td>
          <td style="padding:10px;width:45%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2109.12227.pdf">
              <papertitle>Bringing Generalization to Deep Multi-view Detection</papertitle>
            </a>
            <br>
            Jeet Vora, Swetanjal Dutta, <strong>Kanishk Jain</strong>, Shyamgopal Karthik, Vineet Gandhi
            <br>
            <em>WACV workshop</em>, 2022
            <br>
            <a href="https://arxiv.org/pdf/2109.12227.pdf">pdf</a> / <a href="https://github.com/jeetv/gmvd">code</a>/ <a href="data/mvd.bib">bibtex</a>
            <p></p>
            <p>We find that existing state-of-the-art models show poor generalization by overfitting to a single scene and camera configuration. We formalize three critical forms of generalization and propose experiments to evaluate them.</p>
          </td>
        </tr>
          
        <tr>
          <td style="padding:20px;width:5%;vertical-align:middle">
            <div class="one">
              <div class="two" id='acl_2022_image'><img src='images/acl_2022.jpg' width="180" height="100" class="center"></div>
              <!-- <div class="two" id='acl_2022_image'><img src='images/acl_2022.jpg' class="center"></div> -->
            </div>
          </td>
          <td style="padding:10px;width:45%;vertical-align:middle">
            <a href="https://aclanthology.org/2022.findings-acl.270/">
              <papertitle>Comprehensive Multi-Modal Interactions for Referring Image Segmentation</papertitle>
            </a>
            <br>
            <strong>Kanishk Jain</strong>, Vineet Gandhi
            <br>
            <em>ACL Findings</em>, 2022
            <br>
            <a href="https://aclanthology.org/2022.findings-acl.270.pdf">pdf</a> / <a href="https://github.com/kanji95/SHNET">code</a>/ <a href="data/acl_2022.bib">bibtex</a>
            <p></p>
            <p>We investigate Referring Image Segmentation, which outputs a segmentation map corresponding to the natural language description. We propose a novel architecture to effectively capture all forms of multi-modal interactions synchronously.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:15px;width:5%;vertical-align:middle">
            <div class="one">
              <div class="two" id='iros_2021_image'><img src='images/iros_2021.jpg' width="150" height="150" class="center"></div>
              <!-- <div class="two" id='iros_2021_image'><img src='images/iros_2021.jpg' class="center"></div> -->
            </div>
          </td>
          <td style="padding:10px;width:45%;vertical-align:middle">
            <a href="https://ieeexplore.ieee.org/document/9636172">
              <papertitle>Grounding Linguistic Commands to Navigable Regions</papertitle>
            </a>
            <br>
            <strong>Kanishk Jain*</strong>, Nivedita Rufus*, Unni Krishnan R Nair*, Vineet Gandhi, K Madhava Krishna
            <br>
            <em>IROS</em>, 2021
            <br>
            <a href="https://arxiv.org/pdf/2112.13031.pdf">pdf</a> / <a href="https://github.com/kanji95/Talk2car-Refseg">code</a>/ <a href="data/iros_2021.bib">bibtex</a>
            <p></p>
            <p>We propose a novel visual-grounding-based approach to language-guided navigation which brings interpretability and explainability to Vision Language Navigation task.</p>
          </td>
        </tr>
                
      </td>
    </tr>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
        <br>
        <p align="right"><font size="2">Template Courtesy of <a href="https://people.eecs.berkeley.edu/~barron/" target="_blank">Jon Barron</a></font></p>
        </td>
      </tr>
    </tbody>
  </table>

</body>
</html>

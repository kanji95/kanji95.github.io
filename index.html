<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kanishk Jain</title>
  
  <meta name="author" content="Kanishk Jain">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kanishk Jain</name>
              </p>
              <p>I am an incoming Ph.D. candidate at Mila in Fall 2023, where I'll be advised by <a href="https://www.iro.umontreal.ca/~agrawal/">Prof. Aishwarya Agrawal</a>. <br> I completed my Master's at <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a>, where I was co-advised by <a href="https://www.iiit.ac.in/people/faculty/vgandhi/">Prof. Vineet Gandhi</a> and <a href="https://www.iiit.ac.in/people/faculty/mkrishna/">Prof. K Madhava Krishna</a>. <br> I have worked on Visual Grounding, Language-Guided Autonomous Navigation, Multi-View Detection and Multi-Object Tracking. 
              </p>
              <p style="text-align:center">
                <a href="mailto:kanishk5991@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Kanishk_CV.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=NCVuTTAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/kanji95">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/kanji95/">Linkedin</a> &nbsp/&nbsp
                <a href="https://twitter.com/kanishkjain95">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/webpage-circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/webpage-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in following research topics: learning from multiple data modalities, language understanding in autonomous systems during navigation, explainable deep learning, mutli-object tracking, improving robustness to domain shifts and adversarial attacks, learning in low-data regimes, and ensemble learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:5%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fgvc_2023'><img src='images/roman_2023.png' height="70" class="center"></div>
              </div>
            </td>
            <td style="padding:10px;width:45%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.12363">
                <papertitle>Instance-Level Semantic Maps for Vision Language Navigation</papertitle>
              </a>
              <br>
              Laksh Nanwani, Anmol Agarwal, <strong>Kanishk Jain</strong>, Raghav Prabhakar, Aaron Monis, Aditya Mathur, Krishna Murthy, Abdul Hafez, Vineet Gandhi, K. Madhava Krishna
              <br>
              <em>ROMAN</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2305.12363.pdf">pdf</a> / <a href="data/roman_2023.bib">bibtex</a>
              <p></p>
              <p>We introduce a novel instance-focused scene representation for indoor settings, enabling seamless language-based navigation across various environments. Our representation accommodates language commands that refer to specific instances within the environment.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:5%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fgvc_2023'><img src='images/fgvc10_2023.png' width="120" height="80" class="center"></div>
              </div>
            </td>
            <td style="padding:10px;width:45%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1Ei1z2tbpmjjB197ZNVCBxOB4S9fUr-xH/view">
                <papertitle>Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification</papertitle>
              </a>
              <br>
              <strong>Kanishk Jain</strong>, Shyamgopal Karthik, Vineet Gandhi
              <br>
              <em>CVPR Workshop</em>, 2023
              <br>
              <a href="https://drive.google.com/file/d/1Ei1z2tbpmjjB197ZNVCBxOB4S9fUr-xH/view">pdf</a> / <a href="https://github.com/kanji95/Hierarchical-Ensembles">code</a> / <a href="data/fgvc_2023.bib">bibtex</a>
              <p></p>
              <p>We investigate the problem of reducing mistake severity for fine-grained classification. Our novel approach of Hierarchical Ensembles (HiE)  utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse-grained predictions.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:5%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vln_2022_image'><img src='images/vln_2022.jpg' width="150" height="150" class="center"></div>
              </div>
            </td>
            <td style="padding:10px;width:45%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2209.11972">
                <papertitle>Ground then Navigate: Language-guided Navigation in Dynamic Scenes</papertitle>
              </a>
              <br>
              <strong>Kanishk Jain*</strong>, Varun Chhangani*, Amogh Tiwari, K Madhava Krishna, Vineet Gandhi
              <br>
              <em>ICRA</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2209.11972.pdf">pdf</a> / <a href="data/vln_2022.bib">bibtex</a>
              <p></p>
              <p>We investigate the Vision-and-Language Navigation problem in the context of autonomous driving in outdoor settings. We explicitly ground the navigable regions corresponding to the textual command and use them directly as guidance for the navigation stack.</p>
            </td>
          </tr>

        <tr>
          <td style="padding:20px;width:5%;vertical-align:middle">
            <div class="one">
              <div class="two" id='mvd_image'><img src='images/mvd.jpg' width="180" height="100" class="center"></div>
              <!-- <div class="two" id='acl_2022_image'><img src='images/acl_2022.jpg' class="center"></div> -->
            </div>
          </td>
          <td style="padding:10px;width:45%;vertical-align:middle">
            <a href="https://openaccess.thecvf.com/content/WACV2023W/RWS/html/Vora_Bringing_Generalization_to_Deep_Multi-View_Pedestrian_Detection_WACVW_2023_paper.html">
              <papertitle>Bringing Generalization to Deep Multi-view Detection</papertitle>
            </a>
            <br>
            Jeet Vora, Swetanjal Dutta, <strong>Kanishk Jain</strong>, Shyamgopal Karthik, Vineet Gandhi
            <br>
            <em>WACV workshop</em>, 2023
            <br>
            <a href="https://openaccess.thecvf.com/content/WACV2023W/RWS/papers/Vora_Bringing_Generalization_to_Deep_Multi-View_Pedestrian_Detection_WACVW_2023_paper.pdf">pdf</a> / <a href="https://github.com/jeetv/gmvd">code</a>/ <a href="data/mvd.bib">bibtex</a>
            <p></p>
            <p>We find that existing state-of-the-art models show poor generalization by overfitting to a single scene and camera configuration. We formalize three critical forms of generalization and propose experiments to evaluate them.</p>
          </td>
        </tr>
          
        <tr>
          <td style="padding:20px;width:5%;vertical-align:middle">
            <div class="one">
              <div class="two" id='acl_2022_image'><img src='images/acl_2022.jpg' width="180" height="100" class="center"></div>
              <!-- <div class="two" id='acl_2022_image'><img src='images/acl_2022.jpg' class="center"></div> -->
            </div>
          </td>
          <td style="padding:10px;width:45%;vertical-align:middle">
            <a href="https://aclanthology.org/2022.findings-acl.270/">
              <papertitle>Comprehensive Multi-Modal Interactions for Referring Image Segmentation</papertitle>
            </a>
            <br>
            <strong>Kanishk Jain</strong>, Vineet Gandhi
            <br>
            <em>ACL Findings</em>, 2022
            <br>
            <a href="https://aclanthology.org/2022.findings-acl.270.pdf">pdf</a> / <a href="https://github.com/kanji95/SHNET">code</a>/ <a href="data/acl_2022.bib">bibtex</a>
            <p></p>
            <p>We investigate Referring Image Segmentation, which outputs a segmentation map corresponding to the natural language description. We propose a novel architecture to effectively capture all forms of multi-modal interactions synchronously.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:15px;width:5%;vertical-align:middle">
            <div class="one">
              <div class="two" id='iros_2021_image'><img src='images/iros_2021.jpg' width="150" height="150" class="center"></div>
              <!-- <div class="two" id='iros_2021_image'><img src='images/iros_2021.jpg' class="center"></div> -->
            </div>
          </td>
          <td style="padding:10px;width:45%;vertical-align:middle">
            <a href="https://ieeexplore.ieee.org/document/9636172">
              <papertitle>Grounding Linguistic Commands to Navigable Regions</papertitle>
            </a>
            <br>
            <strong>Kanishk Jain*</strong>, Nivedita Rufus*, Unni Krishnan R Nair*, Vineet Gandhi, K Madhava Krishna
            <br>
            <em>IROS</em>, 2021
            <br>
            <a href="https://arxiv.org/pdf/2112.13031.pdf">pdf</a> / <a href="https://github.com/kanji95/Talk2car-Refseg">code</a>/ <a href="data/iros_2021.bib">bibtex</a>
            <p></p>
            <p>We propose a novel visual-grounding-based approach to language-guided navigation which brings interpretability and explainability to Vision Language Navigation task.</p>
          </td>
        </tr>
                
      </td>
    </tr>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
        <br>
        <p align="right"><font size="2">Template Courtesy of <a href="https://people.eecs.berkeley.edu/~barron/" target="_blank">Jon Barron</a></font></p>
        </td>
      </tr>
    </tbody>
  </table>

</body>
</html>
